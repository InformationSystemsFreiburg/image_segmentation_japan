{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Instance Image Segmentation for Window and Building Detection with detectron2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKMrtSRdiuBi",
        "colab_type": "text"
      },
      "source": [
        "# Tutorial on how to use instance image segmentation for window and building recognition\n",
        "\n",
        "This small tutorial is targeted at researchers that have basic machine learning and Python programming skills that want to implement instance image segmentation for further use in their models. detectron2 is still under heavy development and as of January 2020 usable with Windows without some code changes [that are explained here ](https://github.com/InformationSystemsFreiburg/image_segmentation_japan). Instead of using detectron2 on a locaal machine, you can also use Google Colab and a free GPU from Google for your models. The GPU is either an Nvidia K50 or an T4, both of which are powerfull enough to train detectron2 models. **Computation time on Google Colab is limited to 12 hours**.\n",
        "\n",
        "The first part of this tutorials is based on the beginners tutorial of detectron2, the second part and third part come from the research stay of [Markus Rosenfelder](https://www.is.uni-freiburg.de/mitarbeiter-en/team/markus-rosenfelder) at [GCP NIES](https://www.cger.nies.go.jp/gcp/) in Tsukuba."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScmJSo9kAOHr",
        "colab_type": "text"
      },
      "source": [
        "## Part 1 Installation and setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT5jONS7j7Yl",
        "colab_type": "text"
      },
      "source": [
        "### Installation within Google Colab \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pruZ_Mu9kOle",
        "colab_type": "text"
      },
      "source": [
        "First, check what kind of GPU Google is providing you in the current session. You will find the GPU model name in the third row of the first column. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyYgvvHYkcxa",
        "colab_type": "code",
        "outputId": "5d425494-665f-4f3b-d3c9-2d0e78f918fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jan 28 04:24:57 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyYe9406mJie",
        "colab_type": "text"
      },
      "source": [
        "Now onto the installation of all needed packages. Please keep in mind that this will take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0elMZ3Vj9nc",
        "colab_type": "code",
        "outputId": "e894ae70-d77d-4f11-be1b-1036914f63c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "source": [
        "# install dependencies:\n",
        "# (use +cu100 because colab is on CUDA 10.0)\n",
        "!pip install -U torch==1.4+cu100 torchvision==0.5+cu100 -f https://download.pytorch.org/whl/torch_stable.html \n",
        "!pip install cython pyyaml==5.1\n",
        "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "import torch, torchvision\n",
        "torch.__version__\n",
        "!gcc --version\n",
        "# opencv is pre-installed on colab"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.4+cu100\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu100/torch-1.4.0%2Bcu100-cp36-cp36m-linux_x86_64.whl (723.9MB)\n",
            "\u001b[K     |████████████████▉               | 379.7MB 693kB/s eta 0:08:17"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIeGm77PkqYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install detectron2:\n",
        "!git clone https://github.com/facebookresearch/detectron2 detectron2_repo\n",
        "!pip install -e detectron2_repo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0WcXod3lTks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You may need to restart your runtime prior to this, to let your installation take effect\n",
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBJILiULmkr2",
        "colab_type": "text"
      },
      "source": [
        "### Running a pretrained model\n",
        "\n",
        "In this chapter we will run inference on a pretrained and prelabeled model to see if our setup has been succesful so far. For this we will download an image and run inference on it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZ73iZkRmVMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://images.cocodataset.org/val2017/000000439715.jpg -O input.jpg\n",
        "im = cv2.imread(\"./input.jpg\")\n",
        "cv2_imshow(im)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3hCFLGTm0Dl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cfg = get_cfg()\n",
        "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
        "# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "outputs = predictor(im)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHRDuUSdm4nW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# look at the outputs. See https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification\n",
        "outputs[\"instances\"].pred_classes\n",
        "outputs[\"instances\"].pred_boxes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkQ0BKCSm61f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can use `Visualizer` to draw the predictions on the image.\n",
        "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "cv2_imshow(v.get_image()[:, :, ::-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEq4JM8PAW-n",
        "colab_type": "text"
      },
      "source": [
        "## Part 2 - Training and Inferencing (detecting windows and buildings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUnyPgKDnBLr",
        "colab_type": "text"
      },
      "source": [
        "### Creating a custom dataset with VGG Image Annotator\n",
        "Before continuing with programming, one important step has to be learned: how to annotate images efficiently. For this, we will use the [VGG Image Annotator](http://www.robots.ox.ac.uk/~vgg/software/via/via.html). Please keep in mind that you can also [download the annotator](http://www.robots.ox.ac.uk/~vgg/software/via/) for free and use it from your local machine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHEdx9Hdn3gX",
        "colab_type": "text"
      },
      "source": [
        "On your local machine, do the following:\n",
        "\n",
        "- Create a folder named `buildings`\n",
        "- within this folder, create two folders: `val` and `train`\n",
        "- Open the `VGG Annotator` either locally or via the URL mentioned above.\n",
        "- Short introductions on how to use the tool:\n",
        "  - Go to settings and specify the default path to where your train folder is located, example: `../data/buildings/train/` \n",
        "  - create a new attributes called `class`\n",
        "  - set this attribute to `checkbox`\n",
        "  - add `building` and `window` as options to `class`\n",
        "- save the project\n",
        "- copy images to the `train` and `val` folders\n",
        "- import the images to the `VGG Annotator`\n",
        "- zoom into the image with `CTRL` + `Mousewheel`\n",
        "- select the `polygon region shape` tool and start with marking the `windows`\n",
        "- after a polygon is finished, press `Enter` to save it\n",
        "- after all `window` polygons are created, create the `building` polygons\n",
        "- press `Spacebar` to open the annotations\n",
        "- specify the correct `class` to each polygon\n",
        "- after an image is done, save the project\n",
        "- after all images are done, export the annotations to `train` as .json files and rename them to `via_region_data.json`\n",
        "- do all of the above steps also for the validation data\n",
        "\n",
        "The resulting annotated image should look similar to below. An image from Shinagawa takes on average 20-60 minutes to annotate, depending on the amount of buildings and windows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptr-Ma1Ho_Rr",
        "colab_type": "text"
      },
      "source": [
        "![VGG example](https://github.com/InformationSystemsFreiburg/image_segmentation_japan/raw/master/vgg_annotator.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8G5vOPD_gzt",
        "colab_type": "text"
      },
      "source": [
        "### Download the data from GitHub\n",
        "\n",
        "For this tutorial we will use the 112 images already annotated from the [GitHub repository](https://github.com/InformationSystemsFreiburg/image_segmentation_japan). We use `wget` and `unzip` to download the data and unzip it. The data is now located at `/content/train/` and `/content/val/`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbONuHkanFsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/InformationSystemsFreiburg/image_segmentation_japan/raw/master/buildings.zip\n",
        "!unzip buildings.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk1Zg56EAosf",
        "colab_type": "text"
      },
      "source": [
        "### Import more packages\n",
        "\n",
        "For the following code, we need to import additional packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0YMcUwIpvFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import random\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "from detectron2.structures import BoxMode\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.utils.visualizer import ColorMode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdUBdDdpAztG",
        "colab_type": "text"
      },
      "source": [
        "### Read the output `JSON`-file from the VGG Image Annotator\n",
        "\n",
        "This function is needed to read the annotations for all the images correctly and convert into a format that is usable by `detectron2`. If you have additional information, like for example a `building id` or other target classes, you need to change the function accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1JuRR2r_SIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_building_dicts(img_dir):\n",
        "    \"\"\"This function loads the JSON file created with the annotator and converts it to \n",
        "    the detectron2 metadata specifications.\n",
        "    \"\"\"\n",
        "    # load the JSON file\n",
        "    json_file = os.path.join(img_dir, \"via_region_data.json\")\n",
        "    with open(json_file) as f:\n",
        "        imgs_anns = json.load(f)\n",
        "\n",
        "    dataset_dicts = []\n",
        "    # loop through the entries in the JSON file\n",
        "    for idx, v in enumerate(imgs_anns.values()):\n",
        "        record = {}\n",
        "        # add file_name, image_id, height and width information to the records\n",
        "        filename = os.path.join(img_dir, v[\"filename\"])\n",
        "        height, width = cv2.imread(filename).shape[:2]\n",
        "\n",
        "        record[\"file_name\"] = filename\n",
        "        record[\"image_id\"] = idx\n",
        "        record[\"height\"] = height\n",
        "        record[\"width\"] = width\n",
        "\n",
        "        annos = v[\"regions\"]\n",
        "\n",
        "        objs = []\n",
        "        # one image can have multiple annotations, therefore this loop is needed\n",
        "        for annotation in annos:\n",
        "            # reformat the polygon information to fit the specifications\n",
        "            anno = annotation[\"shape_attributes\"]\n",
        "            px = anno[\"all_points_x\"]\n",
        "            py = anno[\"all_points_y\"]\n",
        "            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
        "            poly = [p for x in poly for p in x]\n",
        "\n",
        "            region_attributes = annotation[\"region_attributes\"][\"class\"]\n",
        "\n",
        "            # specify the category_id to match with the class.\n",
        "\n",
        "            if \"building\" in region_attributes:\n",
        "                category_id = 1\n",
        "            elif \"window\" in region_attributes:\n",
        "                category_id = 0\n",
        "\n",
        "            obj = {\n",
        "                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
        "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
        "                \"segmentation\": [poly],\n",
        "                \"category_id\": category_id,\n",
        "                \"iscrowd\": 0,\n",
        "            }\n",
        "            objs.append(obj)\n",
        "        record[\"annotations\"] = objs\n",
        "        dataset_dicts.append(record)\n",
        "\n",
        "    return dataset_dicts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euX7pYemBb_D",
        "colab_type": "text"
      },
      "source": [
        "### Prepare the data\n",
        "\n",
        "The data needs to be loaded and prepared, therefore this code is executed for the training and validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHb3r_4pBbqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the data has to be registered within detectron2, once for the train and once for\n",
        "# the val data\n",
        "for d in [\"train\", \"val\"]:\n",
        "    DatasetCatalog.register(\n",
        "        \"buildings_\" + d,\n",
        "        lambda d=d: get_building_dicts(\"/content/\" + d),\n",
        "    )\n",
        "\n",
        "building_metadata = MetadataCatalog.get(\"buildings_train\")\n",
        "\n",
        "dataset_dicts = get_building_dicts(\"/content/train\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_GVLbacBza_",
        "colab_type": "text"
      },
      "source": [
        "### View the input data\n",
        "\n",
        "To check if everything is working as intended, you can view some of the input data before continuing. You should see two images, with bounding boxes around windows and buildings, where the buildings have a `1` as category and windows a `0`. Try it a few times, if you have images in your `JSON`-annotation file that you have not yet annotated, they will show without any annotations. These images will be skipped in the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmAy2YBXBznp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, d in enumerate(random.sample(dataset_dicts, 2)):\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=building_metadata, scale=0.5)\n",
        "    vis = visualizer.draw_dataset_dict(d)\n",
        "\n",
        "    cv2_imshow(vis.get_image()[:, :, ::-1])\n",
        "    # if you want to save the files, uncomment the line below, but keep in mind that \n",
        "    # the folder inputs has to be created first\n",
        "    # plt.savefig(f\"./inputs/input_{i}.jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XPSkU2PCkMa",
        "colab_type": "text"
      },
      "source": [
        "### Configure the `detectron2` model\n",
        "\n",
        "Now we need to configure our `detectron2` model before we can start training. There are more possible parameters to configure, for more information you can visit the [detectron2 documentation](https://detectron2.readthedocs.io/modules/config.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucl76Br1FQi-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cfg = get_cfg()\n",
        "# you can choose alternative models as backbone here\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\n",
        "    \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n",
        "))\n",
        "\n",
        "cfg.DATASETS.TRAIN = (\"buildings_train\",)\n",
        "cfg.DATASETS.TEST = ()\n",
        "cfg.DATALOADER.NUM_WORKERS = 0\n",
        "# if you changed the model above, you need to adapt the following line as well\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\n",
        "    \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n",
        ")  # Let training initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR, 0.00025 seems a good start\n",
        "cfg.SOLVER.MAX_ITER = (\n",
        "    1000  # 1000 iterations is a good start, for better accuracy increase this value\n",
        ")\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = (\n",
        "    512  # (default: 512), select smaller if faster training is needed\n",
        ")\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2  # for the two classes window and building\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7JHiv9sF0oT",
        "colab_type": "text"
      },
      "source": [
        "### Start training\n",
        "\n",
        "The next four lines of code create an output directory, a `trainer` and start training. If you only want to inference with an existing model, skip these four lines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKsONiA2Fzkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB7l2LkMGyEQ",
        "colab_type": "text"
      },
      "source": [
        "### Inferencing for new data\n",
        "\n",
        "For inferencing, we need to load the final model created by the training and load the validation data set, or any kind of data set that you wish to inference on. Also, two folders have to be created within Google Colab. Be aware that depending on the amount of `iterations` and the chosen `threshold`, some or all images may show no predicted annotations. In that case, you have to adapt your configuration accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xBa_9JNH1Nq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir predictions\n",
        "!mkdir output_images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oF56sH5vF8nT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = (\n",
        "    0.70  # set the testing threshold for this model\n",
        ")\n",
        "\n",
        "# load the validation data\n",
        "cfg.DATASETS.TEST = (\"buildings_val\",)\n",
        "# create a predictor\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "start = datetime.now()\n",
        "\n",
        "validation_folder = Path(\"/content/val\")\n",
        "\n",
        "for i, file in enumerate(validation_folder.glob(\"*.jpg\")):\n",
        "    # this loop opens the .jpg files from the val-folder, creates a dict with the file\n",
        "    # information, plots visualizations and saves the result as .pkl files.\n",
        "    file = str(file)\n",
        "    file_name = file.split(\"/\")[-1]\n",
        "    im = cv2.imread(file)\n",
        "\n",
        "    outputs = predictor(im)\n",
        "    output_with_filename = {}\n",
        "    output_with_filename[\"file_name\"] = file_name\n",
        "    output_with_filename[\"file_location\"] = file\n",
        "    output_with_filename[\"prediction\"] = outputs\n",
        "    with open(f\"/content/predictions/predictions_{i}.pkl\", \"wb\") as f:\n",
        "        pickle.dump(output_with_filename, f)\n",
        "    v = Visualizer(\n",
        "        im[:, :, ::-1],\n",
        "        metadata=building_metadata,\n",
        "        scale=1,\n",
        "        instance_mode=ColorMode.IMAGE_BW,  # remove the colors of unsegmented pixels\n",
        "    )\n",
        "\n",
        "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    plt.imshow(v.get_image()[:, :, ::-1])\n",
        "    plt.savefig(f\"/content/output_images/{file_name}\")\n",
        "print(\"Time needed for inferencing:\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r75w2LewJCIP",
        "colab_type": "text"
      },
      "source": [
        "## Part 3 - Processing the prediction results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uFTRmyfJH3G",
        "colab_type": "text"
      },
      "source": [
        "### Importing of additional packages\n",
        "\n",
        "For processing our results, we need to import a few additional packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHimumjXJGVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from pathlib import Path\n",
        "import multiprocessing as mp\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSRU_gNqJWfJ",
        "colab_type": "text"
      },
      "source": [
        "### Set some general colour and font settings\n",
        "\n",
        "We download a font for displaying text on our images and set a few colors. They are in RGBA-format, so change values as you wish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQbVbq7FK0K7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/google/fonts/raw/master/apache/roboto/Roboto-Regular.ttf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7f7N1STJXCu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define fonts and colors\n",
        "font_id = ImageFont.truetype(\"/content/Roboto-Regular.ttf\", 15)\n",
        "font_result = ImageFont.truetype(\"/content/Roboto-Regular.ttf\", 40)\n",
        "text_color = (255, 255, 255, 128)\n",
        "background_bbox_window = (0, 247, 255, 30)\n",
        "background_bbox_building = (255, 167, 14, 30)\n",
        "background_text = (0, 0, 0, 150)\n",
        "background_mask_window = (0, 247, 255, 100)\n",
        "background_mask_building = (255, 167, 14, 100)\n",
        "device = \"cpu\"\n",
        "\n",
        "# this variable is True if you want to plot the output images, False if you only need\n",
        "# the CSV\n",
        "plot_data = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It4e0g4LXyxr",
        "colab_type": "text"
      },
      "source": [
        "### Function to draw a bounding box\n",
        "\n",
        "This function draws a bounding box as well as the window to facade percentage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKZF7Jp2LDq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def draw_bounding_box(img, bounding_box, text, category, id, draw_box=False):\n",
        "    \"\"\"Draws a bounding box onto the image as well as the building ID and the window \n",
        "    percentage.\"\"\"\n",
        "\n",
        "    x = bounding_box[0]\n",
        "    y = bounding_box[3]\n",
        "    text = str(round(text, 2))\n",
        "    draw = ImageDraw.Draw(img, \"RGBA\")\n",
        "    if draw_box:\n",
        "        if category == 0:\n",
        "            draw.rectangle(bounding_box, fill=background_bbox_window, outline=(0, 0, 0))\n",
        "        elif category == 1:\n",
        "            draw.rectangle(\n",
        "                bounding_box, fill=background_bbox_building, outline=(0, 0, 0)\n",
        "            )\n",
        "    w, h = font_id.getsize(id)\n",
        "\n",
        "    draw.rectangle((x, y, x + w, y - h), fill=background_text)\n",
        "    draw.text((x, y - h), id, fill=text_color, font=font_id)\n",
        "    # for buildings, add the window percentage value in the lower right corner\n",
        "    if category == 1:\n",
        "        w, h = font_result.getsize(text)\n",
        "        draw.rectangle((x, y, x + w, y + h), fill=background_text)\n",
        "        draw.text((x, y), text, fill=text_color, font=font_result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVJjPTujX8PO",
        "colab_type": "text"
      },
      "source": [
        "### Function to draw masks\n",
        "\n",
        "This function draws masks to the image. To do that, the binary numpy masks constisting of `True/False` for each pixel value of the image are converted to `RGBA` image files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JciLGdyULjQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def draw_mask(img, mask, category):\n",
        "    \"\"\"Draws a mask onto the image.\"\"\"\n",
        "\n",
        "    img = img.convert(\"RGBA\")\n",
        "\n",
        "    mask_RGBA = np.zeros((mask.shape[0], mask.shape[1], 4), dtype=np.uint8)\n",
        "    if category == 0:\n",
        "        mask_RGBA[mask] = background_mask_window\n",
        "    elif category == 1:\n",
        "        mask_RGBA[mask] = background_mask_building\n",
        "    mask_RGBA[~mask] = [0, 0, 0, 0]\n",
        "    rgb_image = Image.fromarray(mask_RGBA).convert(\"RGBA\")\n",
        "\n",
        "    img = Image.alpha_composite(img, rgb_image)\n",
        "\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb5rzGAbYQQA",
        "colab_type": "text"
      },
      "source": [
        "### Calculate the percentage of window to facade\n",
        "\n",
        "This function takes the results of the predictions and sums up all window-pixels within a building. The sum is then divided by the total amount of building pixel area of the respective building."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgxx48cfLlVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_window_perc(dataset):\n",
        "    \"\"\"Takes a list of prediction dictionaries as input and calculates the percentage of \n",
        "    window to fassade for each building. The result is save to the dataset. For building\n",
        "    data, the actual percentage is saved, for windows, 1.0 is put in.\"\"\"\n",
        "\n",
        "    for i, data in enumerate(dataset):\n",
        "        data[\"window_percentage\"] = 0\n",
        "        data[\"pixel_area\"] = 0 \n",
        "        # loop through building\n",
        "        if data[\"category\"] == 1:\n",
        "            \n",
        "            window_areas = []\n",
        "\n",
        "            building_mask = data[\"mask\"]\n",
        "            building_area = np.sum(data[\"mask\"])\n",
        "\n",
        "            for x in dataset:\n",
        "                # for each building, loop through each window\n",
        "                if x[\"category\"] == 0:\n",
        "                    x[\"window_percentage\"] = 1\n",
        "                    pixels_overlapping = np.sum(x[\"mask\"][building_mask])\n",
        "\n",
        "                    window_areas.append(pixels_overlapping)\n",
        "\n",
        "            window_percentage = sum(window_areas) / building_area\n",
        "            \n",
        "            data[\"window_percentage\"] = window_percentage\n",
        "            data[\"pixel_area\"] = building_area\n",
        "    \n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQW8RVi-YjF3",
        "colab_type": "text"
      },
      "source": [
        "### Save the building information to a CSV\n",
        "\n",
        "This function saves the results for the buildings and their percentage values into a CSV-file, which can then be used for further processing or modelling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKEvk0NpLnqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_csv(dataset):\n",
        "    \"\"\"Takes a list of lists of data dictionaries as input, flattens this list, creates\n",
        "    a DataFrame, removes unnecessary information and saves it as a CSV.\"\"\"\n",
        "\n",
        "    # flatten list\n",
        "    dataset = [item for sublist in dataset for item in sublist]\n",
        "    df = pd.DataFrame(dataset)\n",
        "\n",
        "\n",
        "    # calculate the percentage of the building compared to the total image size\n",
        "    df[\"total_image_area\"] = df[\"image_height\"] * df[\"image_width\"]\n",
        "    df[\"building_area_perc_of_image\"] = df[\"pixel_area\"] / df[\"total_image_area\"]\n",
        "    # keep only specific columns\n",
        "    df = df[\n",
        "        [\n",
        "            \"file_name\",\n",
        "            \"id\",\n",
        "            \"category\",\n",
        "            \"pixel_area\",\n",
        "            \"building_area_perc_of_image\",\n",
        "            \"window_percentage\",\n",
        "        ]\n",
        "    ]\n",
        "    # only keep building information\n",
        "    df = df[df[\"category\"] == 1]\n",
        "    print(df)\n",
        "\n",
        "    df.to_csv(\"/content/result.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDlPBn14YvZ2",
        "colab_type": "text"
      },
      "source": [
        "### Function to process all the data\n",
        "\n",
        "This function processes the results from the prediction and applies all the processing function from above. Bounding boxes and masks are drawn, the window percentage calculated and the resulting dicts with all the information is return."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjOoYVxULqYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_data(file_path, plot_data=plot_data):\n",
        "    \"\"\"Takes an prediction result in form of a .pkl-file and draws the mask and bounding\n",
        "    box information. From these, the percentage of windows to fassade for each building\n",
        "    is calculated and plotted onto the image if plot_data=True.\"\"\"\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        # the following lines of code extract specific data from the prediction-dict\n",
        "        prediction = pickle.load(f)\n",
        "\n",
        "        image_height = prediction[\"prediction\"][\"instances\"].image_size[0]\n",
        "        image_width = prediction[\"prediction\"][\"instances\"].image_size[1]\n",
        "\n",
        "        boxes = (\n",
        "            prediction[\"prediction\"][\"instances\"]\n",
        "            .get_fields()[\"pred_boxes\"]\n",
        "            .tensor.to(device)\n",
        "            .numpy()\n",
        "        )\n",
        "\n",
        "        img = Image.open(prediction[\"file_location\"])\n",
        "        categories = (\n",
        "            prediction[\"prediction\"][\"instances\"]\n",
        "            .get_fields()[\"pred_classes\"]\n",
        "            .to(device)\n",
        "            .numpy()\n",
        "        )\n",
        "        masks = (\n",
        "            prediction[\"prediction\"][\"instances\"]\n",
        "            .get_fields()[\"pred_masks\"]\n",
        "            .to(device)\n",
        "            .numpy()\n",
        "        )\n",
        "\n",
        "        dataset = []\n",
        "        counter_window = 0\n",
        "        counter_building = 0\n",
        "        # create a new data-dict as well as IDs for each building and window within an\n",
        "        # image\n",
        "        for i, box in enumerate(boxes):\n",
        "\n",
        "            data = {}\n",
        "            data[\"file_name\"] = prediction[\"file_name\"]\n",
        "            data[\"file_location\"] = prediction[\"file_location\"]\n",
        "            data[\"image_height\"] = image_height\n",
        "            data[\"image_width\"] = image_width\n",
        "            # category 0 is always a window\n",
        "            if categories[i] == 0:\n",
        "                data[\"id\"] = f\"w_{counter_window}\"\n",
        "                counter_window = counter_window + 1\n",
        "            # category 1 is always a building\n",
        "            elif categories[i] == 1:\n",
        "                data[\"id\"] = f\"b_{counter_building}\"\n",
        "                counter_building = counter_building + 1\n",
        "\n",
        "            data[\"bounding_box\"] = box\n",
        "            data[\"category\"] = categories[i]\n",
        "            data[\"mask\"] = masks[i]\n",
        "            dataset.append(data)\n",
        "\n",
        "        dataset = calculate_window_perc(dataset)\n",
        "\n",
        "        if plot_data:\n",
        "            for i, data in enumerate(dataset):\n",
        "                draw_bounding_box(\n",
        "                    img,\n",
        "                    data[\"bounding_box\"],\n",
        "                    data[\"window_percentage\"],\n",
        "                    data[\"category\"],\n",
        "                    data[\"id\"],\n",
        "                    draw_box=True,\n",
        "                )\n",
        "            for i, data in enumerate(dataset):\n",
        "                img = draw_mask(img, data[\"mask\"], data[\"category\"])\n",
        "            try:\n",
        "              img.save(\n",
        "                  f\"/content/predictions/{data['file_name']}_prediction.png\",\n",
        "                  quality=95,\n",
        "              )\n",
        "            except UnboundLocalError as e:\n",
        "              print(\"no annotations found, skipping\")\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxIdCkthZFV5",
        "colab_type": "text"
      },
      "source": [
        "### Run the processing\n",
        "\n",
        "This last piece of code runs the processing of the results and saves it as a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AEK2XC-LuqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction_folder = Path(\"/content/predictions/\")\n",
        "prediction_list = []\n",
        "start = datetime.now()\n",
        "for i, file in enumerate(prediction_folder.glob(\"*.pkl\")):\n",
        "    file = str(file)\n",
        "    prediction_list.append(file)\n",
        "\n",
        "dataset = []\n",
        "\n",
        "for file_location in prediction_list:\n",
        "\n",
        "  dataset_part = process_data(file_location)\n",
        "  dataset.append(dataset_part)\n",
        "\n",
        "create_csv(dataset)\n",
        "\n",
        "print(datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}